{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3169b58a-ee30-4af5-8635-9d5696981513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "=== 00_audit ===\n",
      "Python 3.12.7 | pandas 2.3.1 | numpy 2.2.6 | sklearn 1.7.1 | xgboost 3.0.4\n",
      "Platform: Windows 11 | Time: 2025-08-20 13:54:33\n",
      "\n",
      "=== Audit files written to: C:\\Users\\alex\\Desktop\\nrfi\\outputs\\audit ===\n",
      "\n",
      " - join_coverage.csv\n",
      " - label_sanity.csv\n",
      " - leakage_flags.csv\n",
      " - left_join_health.csv\n",
      " - null_rates.csv\n",
      " - numeric_ranges.csv\n",
      " - range_flags.csv\n",
      " - table_summaries.csv\n"
     ]
    }
   ],
   "source": [
    "# === 00_audit: end-to-end data audit, exports CSVs into outputs/audit/ ===\n",
    "import sys, yaml, warnings\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 0) Project root + src path\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name.lower() == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "# Optional: autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import print_run_header, read_parquet\n",
    "print_run_header(\"00_audit\")\n",
    "\n",
    "# 1) Config and dirs\n",
    "CFG_PATH = PROJECT_ROOT / \"configs\" / \"config.yaml\"\n",
    "assert CFG_PATH.exists(), f\"Missing config at {CFG_PATH}\"\n",
    "cfg = yaml.safe_load(CFG_PATH.read_text())\n",
    "\n",
    "PROC_DIR = PROJECT_ROOT / cfg[\"paths\"][\"processed_dir\"]\n",
    "OUT_DIR  = PROJECT_ROOT / cfg[\"paths\"][\"outputs_dir\"] / \"audit\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Load available tables\n",
    "def safe_read_parquet(p: Path):\n",
    "    try:\n",
    "        return read_parquet(p)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not read {p}: {e}\")\n",
    "        return None\n",
    "\n",
    "labels_path  = PROC_DIR / \"labels.parquet\"\n",
    "team_path    = PROC_DIR / \"team_features.parquet\"\n",
    "starter_path = PROC_DIR / \"starter_features.parquet\"\n",
    "lineup_path  = PROC_DIR / \"lineup_features.parquet\"\n",
    "pitch_path   = PROC_DIR / \"pitch_features.parquet\"\n",
    "pw_path      = PROC_DIR / \"park_weather_features.parquet\"  # may not exist if weather skipped\n",
    "\n",
    "tables = {\n",
    "    \"labels\":  safe_read_parquet(labels_path),\n",
    "    \"team\":    safe_read_parquet(team_path),\n",
    "    \"starter\": safe_read_parquet(starter_path),\n",
    "    \"lineup\":  safe_read_parquet(lineup_path),\n",
    "    \"pitch\":   safe_read_parquet(pitch_path),\n",
    "    \"pw\":      safe_read_parquet(pw_path) if pw_path.exists() else None\n",
    "}\n",
    "\n",
    "# 3) Helpers\n",
    "def summary_stats(df: pd.DataFrame, name: str) -> dict:\n",
    "    if df is None or len(df)==0:\n",
    "        return dict(table=name, rows=0, cols=0, dup_game_id=np.nan, null_game_id=np.nan)\n",
    "    dup_gid = df[\"game_id\"].duplicated().sum() if \"game_id\" in df.columns else np.nan\n",
    "    null_gid = df[\"game_id\"].isna().sum() if \"game_id\" in df.columns else np.nan\n",
    "    return dict(table=name, rows=int(len(df)), cols=int(df.shape[1]),\n",
    "                dup_game_id=(int(dup_gid) if not pd.isna(dup_gid) else np.nan),\n",
    "                null_game_id=(int(null_gid) if not pd.isna(null_gid) else np.nan))\n",
    "\n",
    "def null_rates(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    if df is None or len(df)==0: return pd.DataFrame()\n",
    "    out = df.isna().mean().rename(\"null_rate\").reset_index().rename(columns={\"index\":\"column\"})\n",
    "    out.insert(0, \"table\", name)\n",
    "    return out\n",
    "\n",
    "def numeric_ranges(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    if df is None or len(df)==0: return pd.DataFrame()\n",
    "    num = df.select_dtypes(include=[\"number\",\"bool\"])\n",
    "    if num.empty: return pd.DataFrame()\n",
    "    desc = num.describe(percentiles=[0.01,0.05,0.95,0.99]).T.reset_index().rename(columns={\"index\":\"column\"})\n",
    "    desc.insert(0, \"table\", name)\n",
    "    return desc\n",
    "\n",
    "def expected_range_flags(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    if df is None or len(df)==0: return pd.DataFrame(columns=[\"table\",\"column\",\"value\",\"lo\",\"hi\",\"game_id\"])\n",
    "    flags = []\n",
    "    rules = [\n",
    "        (\"team_fi_rate_sdt\",         0.00, 1.00),\n",
    "        (\"starter_fi_allow_rate_sdt\",0.00, 1.00),\n",
    "        (\"lineup_top4_ops_prior\",    0.50, 1.20),\n",
    "        (\"park_factor_runs\",         0.70, 1.40),\n",
    "        (\"air_density_proxy\",        0.50, 1.50),\n",
    "    ]\n",
    "    for col, lo, hi in rules:\n",
    "        if col in df.columns:\n",
    "            bad = df[(df[col].notna()) & ((df[col] < lo) | (df[col] > hi))]\n",
    "            if not bad.empty:\n",
    "                for _, r in bad.head(200).iterrows():\n",
    "                    flags.append(dict(table=name, column=col, value=r[col], lo=lo, hi=hi, game_id=r.get(\"game_id\", None)))\n",
    "    return pd.DataFrame(flags)\n",
    "\n",
    "# Robust leakage helpers\n",
    "def _pick_label_time_col(labels: pd.DataFrame) -> str | None:\n",
    "    for c in [\"game_datetime_utc\", \"game_time_utc\"]:\n",
    "        if c in labels.columns: return c\n",
    "    if \"date\" in labels.columns: return \"date\"\n",
    "    return None\n",
    "\n",
    "def _pick_feature_time_col(df: pd.DataFrame) -> str | None:\n",
    "    if df is None or df.empty: return None\n",
    "    dt_cols = [c for c in df.columns if str(df[c].dtype).startswith(\"datetime64\")]\n",
    "    if dt_cols: return dt_cols[0]\n",
    "    name_hits = [c for c in df.columns if (\"time\" in c.lower() or \"datetime\" in c.lower())]\n",
    "    return name_hits[0] if name_hits else None\n",
    "\n",
    "def _to_utc(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, utc=True, errors=\"coerce\")\n",
    "\n",
    "def leakage_check(df: pd.DataFrame, labels: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    if df is None or df.empty or labels is None or labels.empty: return pd.DataFrame()\n",
    "    lab_t = _pick_label_time_col(labels)\n",
    "    feat_t = _pick_feature_time_col(df)\n",
    "    if lab_t is None or feat_t is None: return pd.DataFrame()\n",
    "    L = labels[[\"game_id\", lab_t]].copy()\n",
    "    F = df[[\"game_id\", feat_t]].copy()\n",
    "    if lab_t == \"date\":  # coarse fallback\n",
    "        L[lab_t] = pd.to_datetime(L[lab_t], utc=True, errors=\"coerce\")\n",
    "    L[\"label_time\"]  = _to_utc(L[lab_t])\n",
    "    F[\"feature_time\"]= _to_utc(F[feat_t])\n",
    "    merged = L.merge(F, on=\"game_id\", how=\"left\")\n",
    "    mask = merged[\"feature_time\"].notna() & merged[\"label_time\"].notna() & (merged[\"feature_time\"] >= merged[\"label_time\"])\n",
    "    out = merged.loc[mask, [\"game_id\",\"feature_time\",\"label_time\"]].copy()\n",
    "    out.insert(0, \"table\", name)\n",
    "    return out.head(1000)\n",
    "\n",
    "def join_coverage(labels: pd.DataFrame, parts: dict) -> pd.DataFrame:\n",
    "    \"\"\"Share of label rows that find a match in each feature table by game_id (uses _merge indicator; no suffix assumptions).\"\"\"\n",
    "    if labels is None or labels.empty: return pd.DataFrame(columns=[\"table\",\"join_coverage\"])\n",
    "    base = labels[[\"game_id\"]].drop_duplicates()\n",
    "    rows = []\n",
    "    for nm, df in parts.items():\n",
    "        if nm == \"labels\" or df is None or df.empty or \"game_id\" not in df.columns:\n",
    "            continue\n",
    "        m = base.merge(df[[\"game_id\"]].drop_duplicates(), on=\"game_id\", how=\"left\", indicator=True)\n",
    "        coverage = (m[\"_merge\"] == \"both\").mean()\n",
    "        rows.append(dict(table=nm, join_coverage=round(float(coverage), 4)))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def left_join_health(labels: pd.DataFrame, pieces: list[pd.DataFrame], names: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Track row retention after successive left joins on game_id.\"\"\"\n",
    "    if labels is None or labels.empty: return pd.DataFrame(columns=[\"step\",\"rows_before\",\"rows_after\",\"delta\"])\n",
    "    base = labels[[\"game_id\"]].drop_duplicates()\n",
    "    out=[]\n",
    "    for nm, df in zip(names, pieces):\n",
    "        before = int(len(base))\n",
    "        if df is None or df.empty or \"game_id\" not in df.columns:\n",
    "            out.append(dict(step=nm, rows_before=before, rows_after=before, delta=0))\n",
    "            continue\n",
    "        base = base.merge(df[[\"game_id\"]].drop_duplicates(), on=\"game_id\", how=\"left\")\n",
    "        after = int(base[\"game_id\"].notna().sum())\n",
    "        out.append(dict(step=nm, rows_before=before, rows_after=after, delta=after-before))\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "# 4) Core summaries\n",
    "summaries = pd.DataFrame([summary_stats(tables[k], k) for k in tables.keys()])\n",
    "summaries.to_csv(OUT_DIR / \"table_summaries.csv\", index=False)\n",
    "\n",
    "nulls = pd.concat([null_rates(tables[k], k) for k in tables.keys() if tables[k] is not None], ignore_index=True)\n",
    "nulls.to_csv(OUT_DIR / \"null_rates.csv\", index=False)\n",
    "\n",
    "ranges = pd.concat([numeric_ranges(tables[k], k) for k in tables.keys() if tables[k] is not None], ignore_index=True)\n",
    "ranges.to_csv(OUT_DIR / \"numeric_ranges.csv\", index=False)\n",
    "\n",
    "flags = pd.concat([expected_range_flags(tables[k], k) for k in tables.keys() if tables[k] is not None], ignore_index=True)\n",
    "flags.to_csv(OUT_DIR / \"range_flags.csv\", index=False)\n",
    "\n",
    "# 5) Leakage (robust)\n",
    "leaks_list = [leakage_check(tables.get(n), tables[\"labels\"], n) for n in [\"team\",\"starter\",\"lineup\",\"pitch\",\"pw\"]]\n",
    "leaks = pd.concat([x for x in leaks_list if x is not None and not x.empty], ignore_index=True) if any(\n",
    "    x is not None and not x.empty for x in leaks_list) else pd.DataFrame(columns=[\"table\",\"game_id\",\"feature_time\",\"label_time\"])\n",
    "leaks.to_csv(OUT_DIR / \"leakage_flags.csv\", index=False)\n",
    "\n",
    "# 6) Join coverage & left-join health\n",
    "coverage = join_coverage(tables[\"labels\"], tables)\n",
    "coverage.to_csv(OUT_DIR / \"join_coverage.csv\", index=False)\n",
    "\n",
    "health = left_join_health(\n",
    "    tables[\"labels\"],\n",
    "    [tables.get(nm) for nm in [\"team\",\"starter\",\"lineup\",\"pitch\",\"pw\"]],\n",
    "    [\"team\",\"starter\",\"lineup\",\"pitch\",\"pw\"]\n",
    ")\n",
    "health.to_csv(OUT_DIR / \"left_join_health.csv\", index=False)\n",
    "\n",
    "# 7) Label sanity\n",
    "if tables[\"labels\"] is not None and not tables[\"labels\"].empty:\n",
    "    label_sanity = {\n",
    "        \"rows\": int(len(tables[\"labels\"])),\n",
    "        \"yrfi_rate\": float(tables[\"labels\"][\"yrfi\"].mean()),\n",
    "        \"date_min\": str(pd.to_datetime(tables[\"labels\"][\"date\"]).min()),\n",
    "        \"date_max\": str(pd.to_datetime(tables[\"labels\"][\"date\"]).max()),\n",
    "        \"game_id_dupes\": int(tables[\"labels\"][\"game_id\"].duplicated().sum())\n",
    "    }\n",
    "else:\n",
    "    label_sanity = {\"rows\":0,\"yrfi_rate\":np.nan,\"date_min\":\"\",\"date_max\":\"\",\"game_id_dupes\":np.nan}\n",
    "pd.DataFrame([label_sanity]).to_csv(OUT_DIR / \"label_sanity.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Audit files written to:\", OUT_DIR, \"===\\n\")\n",
    "for f in sorted(OUT_DIR.glob(\"*.csv\")):\n",
    "    print(\" -\", f.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7faec9-422b-4733-a8ab-a2480ef39131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nrfi)",
   "language": "python",
   "name": "nrfi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
