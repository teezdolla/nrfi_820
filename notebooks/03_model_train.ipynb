{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dbfeb72-ca2f-462e-9f3d-72766cdd2a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 03_model_train ===\n",
      "Python 3.12.7 | pandas 2.3.1 | numpy 2.2.6 | sklearn 1.7.1 | xgboost 3.0.4\n",
      "Platform: Windows 11 | Time: 2025-08-20 18:58:52\n",
      "Note: using neutral park/weather fallback (no weather features).\n",
      "Loaded shapes: labels (8680, 8) team (17360, 4) starter (17360, 4) lineup (8680, 3) pitch (8680, 4) pw (8680, 7)\n"
     ]
    }
   ],
   "source": [
    "# === 03_model_train: universal header + config + loads ===\n",
    "import sys, yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Make src/ importable regardless of where you opened the notebook\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name.lower() == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "# 2) Autoreload for iterative edits\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 3) Imports\n",
    "from utils import print_run_header, read_parquet\n",
    "print_run_header(\"03_model_train\")\n",
    "\n",
    "# 4) Load config\n",
    "CFG_PATH = PROJECT_ROOT / \"configs\" / \"config.yaml\"\n",
    "assert CFG_PATH.exists(), f\"Missing config at {CFG_PATH}\"\n",
    "cfg = yaml.safe_load(CFG_PATH.read_text())\n",
    "\n",
    "# 5) Load processed artifacts from 02_feature_store\n",
    "PROC_DIR = PROJECT_ROOT / cfg[\"paths\"][\"processed_dir\"]\n",
    "\n",
    "labels_path = PROC_DIR / \"labels.parquet\"\n",
    "team_path   = PROC_DIR / \"team_features.parquet\"\n",
    "starter_path= PROC_DIR / \"starter_features.parquet\"\n",
    "lineup_path = PROC_DIR / \"lineup_features.parquet\"\n",
    "pitch_path  = PROC_DIR / \"pitch_features.parquet\"\n",
    "pw_path     = PROC_DIR / \"park_weather_features.parquet\"  # may not exist if weather was skipped\n",
    "\n",
    "for p in [labels_path, team_path, starter_path, lineup_path, pitch_path]:\n",
    "    assert p.exists(), f\"Missing required file: {p}\"\n",
    "\n",
    "labels = read_parquet(labels_path)\n",
    "team   = read_parquet(team_path)\n",
    "starter= read_parquet(starter_path)\n",
    "lineup = read_parquet(lineup_path)\n",
    "pitch  = read_parquet(pitch_path)\n",
    "\n",
    "# Weather/park may be skipped; build a neutral fallback if absent\n",
    "if pw_path.exists():\n",
    "    pw = read_parquet(pw_path)\n",
    "else:\n",
    "    import pandas as pd\n",
    "    pw = pd.DataFrame({\n",
    "        \"game_id\": labels[\"game_id\"],\n",
    "        \"temp_c\": 20.0,\n",
    "        \"rel_humidity\": 50.0,\n",
    "        \"wind_kph\": 8.0,\n",
    "        \"mslp_hpa\": 1015.0,\n",
    "        \"park_factor_runs\": 1.0,\n",
    "        \"air_density_proxy\": 1.0,\n",
    "    })\n",
    "    print(\"Note: using neutral park/weather fallback (no weather features).\")\n",
    "\n",
    "print(\"Loaded shapes:\",\n",
    "      \"labels\", labels.shape,\n",
    "      \"team\", team.shape,\n",
    "      \"starter\", starter.shape,\n",
    "      \"lineup\", lineup.shape,\n",
    "      \"pitch\", pitch.shape,\n",
    "      \"pw\", pw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe795a58-88c4-48ab-bcd6-4ee74e76cac6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'game_pk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_34988\\860702824.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m markov \u001b[38;5;28;01mimport\u001b[39;00m build_first_inning_prior\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m prior = build_first_inning_prior(labels, team, starter, lineup, pw)\n\u001b[32m      3\u001b[39m prior.describe()\n",
      "\u001b[32m~\\Desktop\\nrfi\\src\\markov.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(labels, team_feat, starter_feat, lineup_feat, parkweather_feat)\u001b[39m\n\u001b[32m     10\u001b[39m     df = labels[[\u001b[33m\"game_id\"\u001b[39m,\u001b[33m\"game_pk\"\u001b[39m]].copy()\n\u001b[32m     11\u001b[39m     x = (\n\u001b[32m     12\u001b[39m         df.merge(team_feat,        on=[\u001b[33m\"game_id\"\u001b[39m,\u001b[33m\"game_pk\"\u001b[39m], how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m     13\u001b[39m           .merge(starter_feat,     on=[\u001b[33m\"game_id\"\u001b[39m,\u001b[33m\"game_pk\"\u001b[39m], how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m           .merge(lineup_feat,      on=[\u001b[33m\"game_id\"\u001b[39m,\u001b[33m\"game_pk\"\u001b[39m], how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m     15\u001b[39m           .merge(parkweather_feat, on=[\u001b[33m\"game_id\"\u001b[39m,\u001b[33m\"game_pk\"\u001b[39m], how=\u001b[33m\"left\"\u001b[39m)\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     17\u001b[39m     mu = (\n",
      "\u001b[32m~\\Desktop\\nrfi\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
      "\u001b[32m~\\Desktop\\nrfi\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32m~\\Desktop\\nrfi\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32m~\\Desktop\\nrfi\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1296\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1297\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1299\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1300\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1301\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32m~\\Desktop\\nrfi\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'game_pk'"
     ]
    }
   ],
   "source": [
    "from markov import build_first_inning_prior\n",
    "prior = build_first_inning_prior(labels, team, starter, lineup, pw)\n",
    "prior.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb84007-e592-44cd-a169-fa6daa85df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, model\n",
    "importlib.reload(model)\n",
    "from model import train_hybrid_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b111d2c-5e65-4a16-8014-fe1e53b69023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import train_hybrid_model\n",
    "artifacts = train_hybrid_model(\n",
    "    labels=labels,\n",
    "    features=[team, starter, lineup, pitch, pw],\n",
    "    prior=prior,\n",
    "    cfg=cfg\n",
    ")\n",
    "artifacts[\"model_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7fe7e7-a435-45f4-a421-8fd89682f6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (nrfi)",
   "language": "python",
   "name": "nrfi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
